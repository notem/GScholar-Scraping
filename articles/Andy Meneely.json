[
    {
        "title": "Taxonomy-Based Human Error Assessment for Senior Software Engineering Students",
        "id": "LFxEgGoAAAAJ:gKiMpY-AVTkC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:gKiMpY-AVTkC",
        "authors": [
            "Benjamin S Meyers",
            "Andrew Meneely"
        ],
        "pub_source": "Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1",
        "pub_date": "2024/3/7",
        "description": "Software engineering (SE) is a complex symphony of development activities spanning multiple engineering phases. Despite best efforts, software engineers experience human errors. Human error theory from psychology has been studied in the context of SE, but human error assessment has yet to be adopted as part of typical post-mortem activities in SE. Our goal in this work is to evaluate an existing Taxonomy of Human Errors in Software Engineering (T.H.E.S.E.) as a learning tool for software engineering students. We conducted a user study involving five SE students at the Rochester Institute of Technology (RIT). In two experimental phases (17 weeks total), participants self-reported 162 human errors that they experienced during software development. Participants' feedback collected via surveys indicates that T.H.E.S.E. is clear, simple to use, and general to all phases of SE. Participants also indicated that\u00a0\u2026"
    },
    {
        "title": "What Happens When We Fuzz? Investigating OSS-Fuzz Bug History",
        "id": "LFxEgGoAAAAJ:gVv57TyPmFsC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:gVv57TyPmFsC",
        "authors": [
            "Brandon N Keller",
            "Benjamin S Meyers",
            "Andrew Meneely"
        ],
        "pub_source": "2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)",
        "pub_date": "2023/5/15",
        "description": "BACKGROUND Software engineers must be vigilant in preventing and correcting vulnerabilities and other critical bugs. In servicing this need, numerous tools and techniques have been developed to assist developers. Fuzzers, by autonomously generating inputs to test programs, promise to save time by detecting memory corruption, input handling, exception cases, and other issues.AIMS The goal of this work is to empower developers to prioritize their quality assurance by analyzing the history of bugs generated by OSS-Fuzz. Specifically, we examined what has happened when a project adopts fuzzing as a quality assurance practice by measuring bug lifespans, learning opportunities, and bug types.METHOD We analyzed 44,102 reported issues made public by OSS-Fuzz prior to March 12, 2022. We traced the Git commit ranges reported by repeated fuzz testing to the source code repositories to identify how\u00a0\u2026",
        "citations": 1
    },
    {
        "title": "Examining penetration tester behavior in the collegiate penetration testing competition",
        "id": "LFxEgGoAAAAJ:5MTHONV0fEkC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:5MTHONV0fEkC",
        "authors": [
            "Benjamin S Meyers",
            "Sultan Fahad Almassari",
            "Brandon N Keller",
            "Andrew Meneely"
        ],
        "pub_source": "ACM Transactions on Software Engineering and Methodology (TOSEM)",
        "pub_date": "2022/4/9",
        "description": "Penetration testing is a key practice toward engineering secure software. Malicious actors have many tactics at their disposal, and software engineers need to know what tactics attackers will prioritize in the first few hours of an attack. Projects like MITRE ATT&CK\u2122 provide knowledge, but how do people actually deploy this knowledge in real situations? A penetration testing competition provides a realistic, controlled environment with which to measure and compare the efficacy of attackers. In this work, we examine the details of vulnerability discovery and attacker behavior with the goal of improving existing vulnerability assessment processes using data from the 2019 Collegiate Penetration Testing Competition (CPTC). We constructed 98 timelines of vulnerability discovery and exploits for 37 unique vulnerabilities discovered by 10 teams of penetration testers. We grouped related vulnerabilities together by mapping\u00a0\u2026",
        "citations": 10
    },
    {
        "title": "How Long Do Vulnerabilities Live in the Code? A {Large-Scale} Empirical Measurement Study on {FOSS} Vulnerability Lifetimes",
        "id": "LFxEgGoAAAAJ:lvd772isFD0C",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:lvd772isFD0C",
        "authors": [
            "Nikolaos Alexopoulos",
            "Manuel Brack",
            "Jan Philipp Wagner",
            "Tim Grube",
            "Max M\u00fchlh\u00e4user"
        ],
        "pub_source": "31st USENIX Security Symposium (USENIX Security 22)",
        "pub_date": "2022",
        "description": "How long do vulnerabilities live in the repositories of large, evolving projects? Although the question has been identified as an interesting problem by the software community in online forums, it has not been investigated yet in adequate depth and scale, since the process of identifying the exact point in time when a vulnerability was introduced is particularly cumbersome. In this paper, we provide an automatic approach for accurately estimating how long vulnerabilities remain in the code (their lifetimes). Our method relies on the observation that while it is difficult to pinpoint the exact point of introduction for one vulnerability, it is possible to accurately estimate the average lifetime of a large enough sample of vulnerabilities, via a heuristic approach.",
        "citations": 16
    },
    {
        "title": "Who are vulnerability reporters? a large-scale empirical study on floss",
        "id": "LFxEgGoAAAAJ:u-coK7KVo8oC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:u-coK7KVo8oC",
        "authors": [
            "Nikolaos Alexopoulos",
            "Andrew Meneely",
            "Dorian Arnouts",
            "Max M\u00fchlh\u00e4user"
        ],
        "pub_source": "Proceedings of the 15th ACM/IEEE international symposium on empirical software engineering and measurement (ESEM)",
        "pub_date": "2021/10/11",
        "description": "(Background) Software vulnerabilities pose a serious threat to the security of computer systems. Hence, there is a constant race for defenders to find and patch them before attackers are able to exploit them. Measuring different aspects of this process is important in order to better understand it and improve the odds for defenders. (Aims) The human factor of the vulnerability discovery and patching process has received limited attention. Better knowledge of the characteristics of the people and organizations who discover and report security vulnerabilities can considerably enhance our understanding of the process, provide insights regarding the expended effort in vulnerability hunting, contribute to better security metrics, and help guide practical decisions regarding the strategy of projects to attract vulnerability researchers. (Method) In this paper, we present what is, to the best of our knowledge, the first large-scale\u00a0\u2026",
        "citations": 5
    },
    {
        "title": "An automated post-mortem analysis of vulnerability relationships using natural language word embeddings",
        "id": "LFxEgGoAAAAJ:ZzlSgRqYykMC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:ZzlSgRqYykMC",
        "authors": [
            "Benjamin S Meyers",
            "Andrew Meneely"
        ],
        "pub_source": "Procedia Computer Science",
        "pub_date": "2021/1/1",
        "description": "The daily activities of cybersecurity experts and software engineers\u2014code reviews, issue tracking, vulnerability reporting\u2014are constantly contributing to a massive wealth of security-specific natural language. In the case of vulnerabilities, understanding their causes, consequences, and mitigations is essential to learning from past mistakes and writing better, more secure code in the future. Many existing vulnerability assessment methodologies, like CVSS, rely on categorization and numerical metrics to glean insights into vulnerabilities, but these tools are unable to capture the subtle complexities and relationships between vulnerabilities because they do not examine the nuanced natural language artifacts left behind by developers. In this work, we want to discover unexpected relationships between vulnerabilities with the goal of improving upon current practices for post-mortem analysis of vulnerabilities. To that end\u00a0\u2026",
        "citations": 4
    },
    {
        "title": "Characterizing attacker behavior in a cybersecurity penetration testing competition",
        "id": "LFxEgGoAAAAJ:Ehil0879vHcC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:Ehil0879vHcC",
        "authors": [
            "Nuthan Munaiah",
            "Akond Rahman",
            "Justin Pelletier",
            "Laurie Williams",
            "Andrew Meneely"
        ],
        "pub_source": "2019 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)",
        "pub_date": "2019/9/19",
        "description": "Background Inculcating an attacker mindset (i.e. learning to think like an attacker) is an essential skill for engineers and administrators to improve the overall security of software. Describing the approach that adversaries use to discover and exploit vulnerabilities to infiltrate software systems can help inform such an attacker mindset. Aims Our goal is to assist developers and administrators in inculcating an attacker mindset by proposing an approach to codify attacker behavior in cybersecurity penetration testing competition. Method We use an existing multimodal dataset of events captured during the 2018 National Collegiate Penetration Testing Competition (CPTC'18) to characterize the approach a team of attackers used to discover and exploit vulnerabilities. Results We collected 44 events to characterize the approach that one of the participating teams took to discover and exploit seven vulnerabilities. We used the\u00a0\u2026",
        "citations": 36
    },
    {
        "title": "Data-driven insights from vulnerability discovery metrics",
        "id": "LFxEgGoAAAAJ:P7Ujq4OLJYoC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:P7Ujq4OLJYoC",
        "authors": [
            "Nuthan Munaiah",
            "Andrew Meneely"
        ],
        "pub_source": "2019 IEEE/ACM Joint 4th International Workshop on Rapid Continuous Software Engineering and 1st International Workshop on Data-Driven Decisions, Experimentation and Evolution (RCoSE/DDrEE)",
        "pub_date": "2019/5/27",
        "description": "Software metrics help developers discover and fix mistakes. However, despite promising empirical evidence, vulnerability discovery metrics are seldom relied upon in practice. In prior research, the effectiveness of these metrics has typically been expressed using precision and recall of a prediction model that uses the metrics as explanatory variables. These prediction models, being black boxes, may not be perceived as useful by developers. However, by systematically interpreting the models and metrics, we can provide developers with nuanced insights about factors that have led to security mistakes in the past. In this paper, we present a preliminary approach to using vulnerability discovery metrics to provide insightful feedback to developers as they engineer software. We collected ten metrics (churn, collaboration centrality, complexity, contribution centrality, nesting, known offender, source lines of code, # inputs\u00a0\u2026",
        "citations": 9
    },
    {
        "title": "Pragmatic characteristics of security conversations: an exploratory linguistic analysis",
        "id": "LFxEgGoAAAAJ:OP4eGU-M3BUC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:OP4eGU-M3BUC",
        "authors": [
            "Benjamin S Meyers",
            "Nuthan Munaiah",
            "Andrew Meneely",
            "Emily Prud'hommeaux"
        ],
        "pub_source": "2019 IEEE/ACM 12th International Workshop on Cooperative and Human Aspects of Software Engineering (CHASE)",
        "pub_date": "2019/5/27",
        "description": "Experts suggest that engineering secure software requires a defensive mindset to be ingrained in developer culture, which could be reflected in conversation. But what does a conversation about software security in a real project look like? Linguists analyze a wide array of characteristics: lexical, syntactic, semantic, and pragmatic. Pragmatics focus on identifying the style and tone of the author's language. If security requires a different mindset, then perhaps this would be reflected in the conversations' pragmatics. Our goal is to characterize the pragmatic features of conversations about security so that developers can be more informed about communication strategies regarding security concerns. We collected and annotated a corpus of conversations from 415,041 bug reports in the Chromium project. We examined five linguistic metrics related to pragmatics: formality, informativeness, implicature, politeness, and\u00a0\u2026",
        "citations": 6
    },
    {
        "title": "Systematization of Vulnerability Discovery Knowledge: Review Protocol",
        "id": "LFxEgGoAAAAJ:F9fV5C73w3QC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:F9fV5C73w3QC",
        "authors": [
            "Nuthan Munaiah",
            "Andrew Meneely"
        ],
        "pub_source": "arXiv preprint arXiv:1902.03331",
        "pub_date": "2019/2/8",
        "description": "In this report, we describe the review protocol that will guide the systematic review of the literature in metrics-based discovery of vulnerabilities. The protocol have been developed in adherence with the guidelines for performing Systematic Literature Reviews in Software Engineering prescribed by Kitchenham and Charters.",
        "citations": 1
    },
    {
        "title": "A cybersecurity dataset derived from the national collegiate penetration testing competition",
        "id": "LFxEgGoAAAAJ:GFxP56DSvIMC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:GFxP56DSvIMC",
        "authors": [
            "Nuthan Munaiah",
            "Justin Pelletier",
            "Shau-Hsuan Su",
            "S Jay Yang",
            "Andrew Meneely"
        ],
        "pub_source": "HICSS Symposium on cybersecurity big data analytics",
        "pub_date": "2019/1",
        "description": "Developers, and administrators, can benefit from inculcating an attacker mindset to foreshadow potential security flaws is software systems as they are developed and/or administered. However, the lack of empirical data about real cyberattacks poses a challenge to understanding attacker behavior. In this paper, we describe a dataset captured during the recently held National Collegiate Penetration Testing Competition (CPTC) which can provide some insight into typical attackers\u2019 modus operandi. The competition had nine teams competing to compromise an enterprise cyberinfrastructure advertised as belonging to a fictitious ride sharing organization. The dataset contains an export of over 500 million log events (with a compressed size of over 8.4 GB) and 99 virtual machines (with a compressed size of over 135 GB). The dataset is, to the best of our knowledge, the first of its kind providing insights at the application level from the perspectives of both the attacker and the victim.",
        "citations": 14
    },
    {
        "title": "Attack surface definitions: A systematic literature review",
        "id": "LFxEgGoAAAAJ:LhH-TYMQEocC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:LhH-TYMQEocC",
        "authors": [
            "Christopher Theisen",
            "Nuthan Munaiah",
            "Mahran Al-Zyoud",
            "Jeffrey C Carver",
            "Andrew Meneely",
            "Laurie Williams"
        ],
        "pub_source": "Information and Software Technology 104, 94-103",
        "pub_date": "2018/12/1",
        "description": "ContextMichael Howard conceptualized the attack surface of a software system as a metaphor for risk assessment during the development and maintenance of software. While the phrase attack surface is used in a variety of contexts in cybersecurity, professionals have different conceptions of what the phrase means.ObjectiveThe goal of this systematic literature review is to aid researchers and practitioners in reasoning about security in terms of attack surface by exploring various definitions of the phrase attack surface.MethodWe reviewed 644 works from prior literature, including research papers, magazine articles, and technical reports, that use the phrase attack surface and categorized them into those that provided their own definition; cited another definition; or expected the reader to intuitively understand the phrase.ResultsIn our study, 71% of the papers used the phrase without defining it or citing another paper\u00a0\u2026",
        "citations": 64
    },
    {
        "title": "A dataset for identifying actionable feedback in collaborative software development",
        "id": "LFxEgGoAAAAJ:yqoGN6RLRZoC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:yqoGN6RLRZoC",
        "authors": [
            "Benjamin S Meyers",
            "Nuthan Munaiah",
            "Emily Prud\u2019Hommeaux",
            "Andrew Meneely",
            "Josephine Wolff",
            "Cecilia Ovesdotter Alm",
            "Pradeep Murukannaiah"
        ],
        "pub_source": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
        "pub_date": "2018/7",
        "description": "Software developers and testers have long struggled with how to elicit proactive responses from their coworkers when reviewing code for security vulnerabilities and errors. For a code review to be successful, it must not only identify potential problems but also elicit an active response from the colleague responsible for modifying the code. To understand the factors that contribute to this outcome, we analyze a novel dataset of more than one million code reviews for the Google Chromium project, from which we extract linguistic features of feedback that elicited responsive actions from coworkers. Using a manually-labeled subset of reviewer comments, we trained a highly accurate classifier to identify acted-upon comments (AUC= 0.85). Our results demonstrate the utility of our dataset, the feasibility of using NLP for this new task, and the potential of NLP to improve our understanding of how communications between colleagues can be authored to elicit positive, proactive responses.",
        "citations": 9
    },
    {
        "title": "A domain-independent model for identifying security requirements",
        "id": "LFxEgGoAAAAJ:KbBQZpvPDL4C",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:KbBQZpvPDL4C",
        "authors": [
            "Nuthan Munaiah",
            "Andrew Meneely",
            "Pradeep K Murukannaiah"
        ],
        "pub_source": "2017 IEEE 25th International Requirements Engineering Conference (RE)",
        "pub_date": "2017/9/4",
        "description": "Existing work on identifying security requirements relies on training binary classification models using domain-specific data sets to achieve a high accuracy. Considering that domain-specific data sets are often not readily available, we propose a domain-independent model for classifying security requirements based on two key ideas. First, we train our model on the description of weaknesses from the Common Weakness Enumeration (CWE) data set. Although CWE does not describe requirements, it describes security weaknesses that are manifestations of unrealized security requirements. Second, we exploit a one-class classification model that relies only on positive samples (description of weaknesses in CWE), eliminating the need for negative samples, collecting which can be nontrivial.We evaluated our model on three industrial requirements documents from different domains. We found that a One-Class\u00a0\u2026",
        "citations": 24
    },
    {
        "title": "Do bugs foreshadow vulnerabilities? An in-depth study of the chromium project",
        "id": "LFxEgGoAAAAJ:cWzG1nlazyYC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:cWzG1nlazyYC",
        "authors": [
            "Nuthan Munaiah",
            "Felivel Camilo",
            "Wesley Wigham",
            "Andrew Meneely",
            "Meiyappan Nagappan"
        ],
        "pub_source": "Empirical Software Engineering",
        "pub_date": "2017/6",
        "description": "As developers face an ever-increasing pressure to engineer secure software, researchers are building an understanding of security-sensitive bugs (i.e. vulnerabilities). Research into mining software repositories has greatly increased our understanding of software quality via empirical study of bugs. Conceptually, however, vulnerabilities differ from bugs: they represent an abuse of functionality as opposed to insufficient functionality commonly associated with traditional, non-security bugs. We performed an in-depth analysis of the Chromium project to empirically examine the relationship between bugs and vulnerabilities. We mined 374,686 bugs and 703 post-release vulnerabilities over five Chromium releases that span six years of development. We used logistic regression analysis, ranking analysis, bug type classifications, developer experience, and vulnerability severity metrics to examine the\u00a0\u2026",
        "citations": 45
    },
    {
        "title": "Natural Language Insights from Code Reviews that Missed a Vulnerability: A Large Scale Study of Chromium",
        "id": "LFxEgGoAAAAJ:3htObqc8RwsC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:3htObqc8RwsC",
        "authors": [
            "Nuthan Munaiah",
            "Benjamin S Meyers",
            "Cecilia O Alm",
            "Andrew Meneely",
            "Pradeep K Murukannaiah",
            "Emily Prud\u2019hommeaux",
            "Josephine Wolff",
            "Yang Yu"
        ],
        "pub_source": "Engineering Secure Software and Systems: 9th International Symposium, ESSoS 2017, Bonn, Germany, July 3-5, 2017, Proceedings 9",
        "pub_date": "2017",
        "description": "Engineering secure software is challenging. Software development organizations leverage a host of processes and tools to enable developers to prevent vulnerabilities in software. Code reviewing is one such approach which has been instrumental in improving the overall quality of a software system. In a typical code review, developers critique a proposed change to uncover potential vulnerabilities. Despite best efforts by developers, some vulnerabilities inevitably slip through the reviews. In this study, we characterized linguistic features\u2014inquisitiveness, sentiment and syntactic complexity\u2014of conversations between developers in a code review, to identify factors that could explain developers missing a vulnerability. We used natural language processing to collect these linguistic features from 3,994,976 messages in 788,437 code reviews from the Chromium project. We collected 1,462 Chromium\u00a0\u2026",
        "citations": 16
    },
    {
        "title": "Examining the relationship between security metrics and user ratings of mobile apps: A case study",
        "id": "LFxEgGoAAAAJ:OcBU2YAGkTUC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:OcBU2YAGkTUC",
        "authors": [
            "Daniel E Krutz",
            "Nuthan Munaiah",
            "Andrew Meneely",
            "Samuel A Malachowsky"
        ],
        "pub_source": "Proceedings of the International Workshop on App Market Analytics",
        "pub_date": "2016/11/14",
        "description": "The success or failure of a mobile application (`app') is largely determined by user ratings. Users frequently make their app choices based on the ratings of apps in comparison with similar, often competing apps. Users also expect apps to continually provide new features while maintaining quality, or the ratings drop. At the same time apps must also be secure, but is there a historical trade-off between security and ratings? Or are app store ratings a more all-encompassing measure of product maturity? We used static analysis tools to collect security-related metrics in 38,466 Android apps from the Google Play store. We compared the rate of an app's permission misuse, number of requested permissions, and Androrisk score, against its user rating.   We found that high-rated apps have statistically significantly higher security risk metrics than low-rated apps. However, the correlations are weak. This result supports the\u00a0\u2026",
        "citations": 17
    },
    {
        "title": "The impact of cross-platform development approaches for mobile applications from the user's perspective",
        "id": "LFxEgGoAAAAJ:yFnVuubrUp4C",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:yFnVuubrUp4C",
        "authors": [
            "Iv\u00e1n Tactuk Mercado",
            "Nuthan Munaiah",
            "Andrew Meneely"
        ],
        "pub_source": "Proceedings of the International Workshop on App Market Analytics",
        "pub_date": "2016/11/14",
        "description": "Mobile app developers today have a hard decision to make: to independently develop native apps for different operating systems or to develop an app that is cross-platform compatible. The availability of different tools and approaches to support cross-platform app development only makes the decision harder. In this study, we used user reviews of apps to empirically understand the relationship (if any) between the approach used in the development of an app and its perceived quality. We used Natural Language Processing (NLP) models to classify 787,228 user reviews of the Android version and iOS version of 50 apps as complaints in one of four quality concerns: performance, usability, security, and reliability. We found that hybrid apps (on both Android and iOS platforms) tend to be more prone to user complaints than interpreted/generated apps. In a study of Facebook, an app that underwent a change in\u00a0\u2026",
        "citations": 65
    },
    {
        "title": "Vulnerability severity scoring and bounties: Why the disconnect?",
        "id": "LFxEgGoAAAAJ:ODE9OILHJdcC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:ODE9OILHJdcC",
        "authors": [
            "Nuthan Munaiah",
            "Andrew Meneely"
        ],
        "pub_source": "Proceedings of the 2nd International Workshop on Software Analytics",
        "pub_date": "2016/11/13",
        "description": "The Common Vulnerability Scoring System (CVSS) is the de facto standard for vulnerability severity measurement today and is crucial in the analytics driving software fortification. Required by the U.S. National Vulnerability Database, over 75,000 vulnerabilities have been scored using CVSS. We compare how the CVSS correlates with another, closely-related measure of security impact: bounties. Recent economic studies of vulnerability disclosure processes show a clear relationship between black market value and bounty payments. We analyzed the CVSS scores and bounty awarded for 703 vulnerabilities across 24 products. We found a weak (Spearman's \u03c1 = 0.34) correlation between CVSS scores and bounties, with CVSS being more likely to underestimate bounty. We believe such a negative result is a cause for concern. We investigated why these measurements were so discordant by (a) analyzing the\u00a0\u2026",
        "citations": 49
    },
    {
        "title": "Beyond the attack surface: Assessing security risk with random walks on call graphs",
        "id": "LFxEgGoAAAAJ:bKqednn6t2AC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:bKqednn6t2AC",
        "authors": [
            "Nuthan Munaiah",
            "Andrew Meneely"
        ],
        "pub_source": "Proceedings of the 2016 ACM Workshop on Software PROtection",
        "pub_date": "2016/10/28",
        "description": "When reasoning about software security, researchers and practitioners use the phrase ``attack surface'' as a metaphor for risk. Enumerate and minimize the ways attackers can break in then risk is reduced and the system is better protected, the metaphor says. But software systems are much more complicated than their surfaces. We propose function- and file-level attack surface metrics---proximity and risky walk---that enable fine-grained risk assessment. Our risky walk metric is highly configurable: we use PageRank on a probability-weighted call graph to simulate attacker behavior of finding or exploiting a vulnerability. We provide evidence-based guidance for deploying these metrics, including an extensive parameter tuning study. We conducted an empirical study on two large open source projects, FFmpeg and Wireshark, to investigate the potential correlation between our metrics and historical post-release\u00a0\u2026",
        "citations": 25
    },
    {
        "title": "Are intrusion detection studies evaluated consistently? a systematic literature review",
        "id": "LFxEgGoAAAAJ:HtS1dXgVpQUC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:HtS1dXgVpQUC",
        "authors": [
            "Nuthan Munaiah",
            "Andrew Meneely",
            "Ryan Wilson",
            "Benjamin Short"
        ],
        "pub_source": "RIT Scholar Works. Sep",
        "pub_date": "2016/9/28",
        "description": "Cyberinfrastructure is increasingly becoming target of a wide spectrum of attacks from Denial of Service to large-scale defacement of the digital presence of an organization. Intrusion Detection System (IDSs) provide administrators a defensive edge over intruders lodging such malicious attacks. However, with the sheer number of different IDSs available, one has to objectively assess the capabilities of different IDSs to select an IDS that meets specific organizational requirements. A prerequisite to enable such an objective assessment is the implicit comparability of IDS literature. In this study, we review IDS literature to understand the implicit comparability of IDS literature from the perspective of metrics used in the empirical evaluation of the IDS. We identified 22 metrics commonly used in the empirical evaluation of IDS and constructed search terms to retrieve papers that mention the metric. We manually reviewed a sample of 495 papers and found 159 of them to be relevant. We then estimated the number of relevant papers in the entire set of papers retrieved from IEEE. We found that, in the evaluation of IDSs, multiple different metrics are used and the trade-off between metrics is rarely considered. In a retrospective analysis of the IDS literature, we found the the evaluation criteria has been improving over time, albeit marginally. The inconsistencies in the use of evaluation metrics may not enable direct comparison of one IDS to another.",
        "citations": 17
    },
    {
        "title": "Security cannot be measured",
        "id": "LFxEgGoAAAAJ:nVrZBo8bIpAC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:nVrZBo8bIpAC",
        "authors": [
            "Andrew Meneely"
        ],
        "pub_source": "Perspectives on Data Science for Software Engineering",
        "pub_date": "2016/1/1",
        "description": "Which of the following is more secure: Windows, Mac OS X, or the White House?All of them have been broken into. They all have security features. A lot of money has been spent on making them secure (some of that money was better spent in some places than in others). They also serve different purposes and have different designs. So which is it? Which is more secure? Or is that a fair comparison at all?"
    },
    {
        "title": "Actionable metrics are better metrics",
        "id": "LFxEgGoAAAAJ:HeT0ZceujKMC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:HeT0ZceujKMC",
        "authors": [
            "Andrew Meneely"
        ],
        "pub_source": "Perspectives on Data Science for Software Engineering",
        "pub_date": "2016/1/1",
        "description": "True, data is just data. But humans use data to lie all the time\u2026 with metrics. Metrics are a huge part of our evidence-based society. Metrics drive what data we collect, determine the exact phrasing of our conclusions, and shape our perspectives. One wrong step with a metric, and we can grossly misunderstand the world around us.",
        "citations": 3
    },
    {
        "title": "An insider threat activity in a software security course",
        "id": "LFxEgGoAAAAJ:4vMrXwiscB8C",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:4vMrXwiscB8C",
        "authors": [
            "Daniel E Krutz",
            "Andrew Meneely",
            "Samuel A Malachowsky"
        ],
        "pub_source": "2015 IEEE Frontiers in Education Conference (FIE)",
        "pub_date": "2015/10/21",
        "description": "Software development teams face a critical threat to the security of their systems: insiders. A malicious insider is a person who violates an authorized level of access in a software system. Unfortunately, when creating software, developers do not typically account for insider threat. Students learning software development are unaware of the impacts of malicious actors and are far too often untrained in prevention methods against them. A few of the defensive mechanisms to protect against insider threats include eliminating system access once an employee leaves an organization, enforcing principle of least privilege, code reviews, and constant monitoring for suspicious activity. At the Department of Software Engineering at the Rochester Institute of Technology, we require a course titled Engineering of Secure Software and have created an activity designed to prepare students for the problem of insider threats. At the\u00a0\u2026",
        "citations": 5
    },
    {
        "title": "Do bugs foreshadow vulnerabilities? a study of the chromium project",
        "id": "LFxEgGoAAAAJ:HIFyuExEbWQC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:HIFyuExEbWQC",
        "authors": [
            "Felivel Camilo",
            "Andrew Meneely",
            "Meiyappan Nagappan"
        ],
        "pub_source": "2015 IEEE/ACM 12th Working Conference on Mining Software Repositories",
        "pub_date": "2015/5/16",
        "description": "As developers face ever-increasing pressure to engineer secure software, researchers are building an understanding of security-sensitive bugs (i.e. Vulnerabilities). Research into mining software repositories has greatly increased our understanding of software quality via empirical study of bugs. However, conceptually vulnerabilities are different from bugs: they represent abusive functionality as opposed to wrong or insufficient functionality commonly associated with traditional, non-security bugs. In this study, we performed an in-depth analysis of the Chromium project to empirically examine the relationship between bugs and vulnerabilities. We mined 374,686 bugs and 703 post-release vulnerabilities over five Chromium releases that span six years of development. Using logistic regression analysis, we examined how various categories of pre-release bugs (e.g. Stability, compatibility, etc.) are associated with post\u00a0\u2026",
        "citations": 84
    },
    {
        "title": "Analyzing Security Data",
        "id": "LFxEgGoAAAAJ:r_AWSJRzSzQC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:r_AWSJRzSzQC",
        "authors": [
            "Andrew Meneely"
        ],
        "pub_source": "The Art and Science of Analyzing Software Data",
        "pub_date": "2015/1/1",
        "description": "Security is a challenging and strange property of software. Security is not about understanding how a customer might use the system; security is about ensuring that an attacker cannot abuse the system. Instead of defining what the system should do, security is about ensuring that system does not do something malicious. As a result, applying traditional software analytics to security leads to some unique challenges and caveats. In this chapter, we will discuss four \u201cgotchas\u201d of analyzing security data, along with vulnerabilities and severity scoring. We will describe a method commonly-used for collecting security data in open source projects. We will describe some of the state-of-the-art in analyzing security data today.Methods: Analyzing Vulnerability Coverage.",
        "citations": 5
    },
    {
        "title": "An empirical investigation of socio-technical code review metrics and security vulnerabilities",
        "id": "LFxEgGoAAAAJ:YOwf2qJgpHMC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:YOwf2qJgpHMC",
        "authors": [
            "Andrew Meneely",
            "Alberto C Rodriguez Tejeda",
            "Brian Spates",
            "Shannon Trudeau",
            "Danielle Neuberger",
            "Katherine Whitlock",
            "Christopher Ketant",
            "Kayla Davis"
        ],
        "pub_source": "Proceedings of the 6th International Workshop on Social Software Engineering",
        "pub_date": "2014/11/17",
        "description": "One of the guiding principles of open source software development is to use crowds of developers to keep a watchful eye on source code. Eric Raymond declared Linus'' Law as \"many eyes make all bugs shallow\", with the socio-technical argument that high quality open source software emerges when developers combine together their collective experience and expertise to review code collaboratively. Vulnerabilities are a particularly nasty set of bugs that can be rare, difficult to reproduce, and require specialized skills to recognize. Does Linus' Law apply to vulnerabilities empirically? In this study, we analyzed 159,254 code reviews, 185,948 Git commits, and 667 post-release vulnerabilities in the Chromium browser project. We formulated, collected, and analyzed various metrics related to Linus' Law to explore the connection between collaborative reviews and vulnerabilities that were missed by the review\u00a0\u2026",
        "citations": 75
    },
    {
        "title": "Teaching Web Engineering using a project component",
        "id": "LFxEgGoAAAAJ:Zph67rFs4hoC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:Zph67rFs4hoC",
        "authors": [
            "Daniel E Krutz",
            "Andrew Meneely"
        ],
        "pub_source": "2013 IEEE Frontiers in Education Conference (FIE)",
        "pub_date": "2013/10/23",
        "description": "Web applications are an intricate part of the world today. Everything from banking to checking our Facebook status may now be done through the use of web applications. Todays students need to balance numerous concerns in order to create a web application that is robust, on time and on budget. At the Department of Software Engineering at the Rochester Institute of Technology, we created a course called Web Engineering. As part of this course, we developed an innovative project component which focused on students following software engineering principles such as elicitation, requirements generation, testing and deployment.",
        "citations": 1
    },
    {
        "title": "When a patch goes bad: Exploring the properties of vulnerability-contributing commits",
        "id": "LFxEgGoAAAAJ:KlAtU1dfN6UC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:KlAtU1dfN6UC",
        "authors": [
            "Andrew Meneely",
            "Harshavardhan Srinivasan",
            "Ayemi Musa",
            "Alberto Rodriguez Tejeda",
            "Matthew Mokary",
            "Brian Spates"
        ],
        "pub_source": "2013 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
        "pub_date": "2013/10/10",
        "description": "Security is a harsh reality for software teams today. Developers must engineer secure software by preventing vulnerabilities, which are design and coding mistakes that have security consequences. Even in open source projects, vulnerable source code can remain unnoticed for years. In this paper, we traced 68 vulnerabilities in the Apache HTTP server back to the version control commits that contributed the vulnerable code originally. We manually found 124 Vulnerability-Contributing Commits (VCCs), spanning 17 years. In this exploratory study, we analyzed these VCCs quantitatively and qualitatively with the over-arching question: \"What could developers have looked for to identify security concerns in this commit?\" Specifically, we examined the size of the commit via code churn metrics, the amount developers overwrite each others' code via interactive churn metrics, exposure time between VCC and fix, and\u00a0\u2026",
        "citations": 109
    },
    {
        "title": "Vulnerability of the day: Concrete demonstrations for software engineering undergraduates",
        "id": "LFxEgGoAAAAJ:MXK_kJrjxJIC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:MXK_kJrjxJIC",
        "authors": [
            "Andrew Meneely",
            "Samuel Lucidi"
        ],
        "pub_source": "2013 35th international conference on software engineering (icse)",
        "pub_date": "2013/5/18",
        "description": "Software security is a tough reality that affects the many facets of our modern, digital world. The pressure to produce secure software is felt particularly strongly by software engineers. Today's software engineering students will need to deal with software security in their profession. However, these students will also not be security experts, rather, they need to balance security concerns with the myriad of other draws of their attention, such as reliability, performance, and delivering the product on-time and on-budget. At the Department of Software Engineering at the Rochester Institute of Technology, we developed a course called Engineering Secure Software, designed for applying security principles to each stage of the software development lifecycle. As a part of this course, we developed a component called Vulnerability of the Day, which is a set of selected example software vulnerabilities. We selected these\u00a0\u2026",
        "citations": 16
    },
    {
        "title": "Validating software metrics: A spectrum of philosophies",
        "id": "LFxEgGoAAAAJ:ufrVoPGSRksC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:ufrVoPGSRksC",
        "authors": [
            "Andrew Meneely",
            "Ben Smith",
            "Laurie Williams"
        ],
        "pub_source": "ACM Transactions on Software Engineering and Methodology (TOSEM) 21 (4), 1-28",
        "pub_date": "2013/2/7",
        "description": "Context. Researchers proposing a new metric have the burden of proof to demonstrate to the research community that the metric is acceptable in its intended use. This burden of proof is provided through the multi-faceted, scientific, and objective process of software metrics validation. Over the last 40 years, however, researchers have debated what constitutes a \u201cvalid\u201d metric. Aim. The debate over what constitutes a valid metric centers on software metrics validation criteria. The objective of this article is to guide researchers in making sound contributions to the field of software engineering metrics by providing a practical summary of the metrics validation criteria found in the academic literature. Method. We conducted a systematic literature review that began with 2,288 papers and ultimately focused on 20 papers. After extracting 47 unique validation criteria from these 20 papers, we performed a comparative analysis\u00a0\u2026",
        "citations": 132
    },
    {
        "title": "Interactive churn metrics: socio-technical variants of code churn",
        "id": "LFxEgGoAAAAJ:0EnyYjriUFMC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:0EnyYjriUFMC",
        "authors": [
            "Andrew Meneely",
            "Oluyinka Williams"
        ],
        "pub_source": "ACM SIGSOFT Software Engineering Notes",
        "pub_date": "2012/11/27",
        "description": "A central part of software quality is finding bugs. One method of finding bugs is by measuring important aspects of the software product and the development process. In recent history, researchers have discovered evidence of a \"code churn\" effect whereby the degree to which a given source code file has changed over time is correlated with faults and vulnerabilities. Computing the code churn metric comes from counting source code differences in version control repositories. However, code churn does not take into account a critical factor of any software development team: the human factor, specifically who is making the changes. In this paper, we introduce a new class of human-centered metrics, \"interactive churn metrics\" as variants of code churn. Using the git blame tool, we identify the most recent developer who changed a given line of code in a file prior to a given revision. Then, for each line changed in a\u00a0\u2026",
        "citations": 31
    },
    {
        "title": "Developing an applied, security-oriented computing curriculum",
        "id": "LFxEgGoAAAAJ:3fE2CSJIrl8C",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:3fE2CSJIrl8C",
        "authors": [
            "Marcin Lukowiak",
            "Andrew Meneely",
            "Stanislaw P Radziszowski",
            "James R Vallino",
            "Christopher A Wood"
        ],
        "pub_source": "2012 ASEE Annual Conference & Exposition",
        "pub_date": "2012/6/10",
        "description": "Developing an Applied, Security-Oriented Computing CurriculumSoftware and hardware security is a reality that all stakeholders must face, from hardwareengineers to software developers to customers. As a direct result, the technology industry isfacing a growing need for graduates who have an understanding of security principles at varyinglevels of abstraction. These graduates will need security-oriented perspectives stemming fromboth theoretical and practical disciplines, including Software Engineering, ComputerEngineering, and Computer Science. Unfortunately, in traditional academic settings, securesoftware and hardware are typically taught by separate departments despite being intertwined inpractice. Consequently, the objective of this initiative is to prepare students to apply a security-oriented awareness to every aspect of hardware and software systems by developing a multi-disciplinary curriculum\u00a0\u2026",
        "citations": 4
    },
    {
        "title": "Appendix B: iTrust electronic health care system case study",
        "id": "LFxEgGoAAAAJ:UebtZRa9Y70C",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:UebtZRa9Y70C",
        "authors": [
            "Andrew Meneely",
            "Ben Smith",
            "Laurie Williams"
        ],
        "pub_source": "Software and Systems Traceability",
        "pub_date": "2012/2/1",
        "description": "Electronic health record (EHR) systems present a formidable \u201ctrustworthiness\u201d challenge because people\u2019s health records, which are transmitted and protected by these systems, are just as valuable to a myriad of attackers as they are to health care practitioners. Major initiatives in EHR adoption and increased sharing of health information raise significant challenges for protecting the privacy of patients\u2019 health information.The United States is pursuing the vision of the National Health Information Network (NHIN) in which the electronic health records of the American people are passed between sometimes-competing health care providers. The American Recovery and Reinvestment Act of 2009 (ARRA, 2009) provides $34 billion of incentives to health care providers to deploy a government-approved EHR. The ARRA will, by 2014, impose penalties on those who do not. As a result, the use of EHR systems is likely to\u00a0\u2026",
        "citations": 74
    },
    {
        "title": "Article 24 (28 pages)-Validating Software Metrics: A Spectrum of Philosophies",
        "id": "LFxEgGoAAAAJ:8k81kl-MbHgC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:8k81kl-MbHgC",
        "authors": [
            "A Meneely",
            "B Smith",
            "L Williams"
        ],
        "pub_source": "ACM Transactions on Software Engineering andMethodology-TOSEM",
        "pub_date": "2012"
    },
    {
        "title": "Does adding manpower also affect quality? an empirical, longitudinal analysis",
        "id": "LFxEgGoAAAAJ:LkGwnXOMwfcC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:LkGwnXOMwfcC",
        "authors": [
            "Andrew Meneely",
            "Pete Rotella",
            "Laurie Williams"
        ],
        "pub_source": "Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering",
        "pub_date": "2011/9/9",
        "description": "With each new developer to a software development team comes a greater challenge to manage the communication, coordination, and knowledge transfer amongst teammates. Fred Brooks discusses this challenge in The Mythical Man-Month by arguing that rapid team expansion can lead to a complex team organization structure. While Brooks focuses on productivity loss as the negative outcome, poor product quality is also a substantial concern. But if team expansion is unavoidable, can any quality impacts be mitigated? Our objective is to guide software engineering managers by empirically analyzing the effects of team size, expansion, and structure on product quality. We performed an empirical, longitudinal case study of a large Cisco networking product over a five year history. Over that time, the team underwent periods of no expansion, steady expansion, and accelerated expansion. Using team-level metrics\u00a0\u2026",
        "citations": 22
    },
    {
        "title": "Socio-technical developer networks: Should we trust our measurements?",
        "id": "LFxEgGoAAAAJ:IjCSPb-OGe4C",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:IjCSPb-OGe4C",
        "authors": [
            "Andrew Meneely",
            "Laurie Williams"
        ],
        "pub_source": "Proceedings of the 33rd international conference on software engineering",
        "pub_date": "2011/5/21",
        "description": "Software development teams must be properly structured to provide effectiv collaboration to produce quality software. Over the last several years, social network analysis (SNA) has emerged as a popular method for studying the collaboration and organization of people working in large software development teams. Researchers have been modeling networks of developers based on socio-technical connections found in software development artifacts. Using these developer networks, researchers have proposed several SNA metrics that can predict software quality factors and describe the team structure. But do SNA metrics measure what they purport to measure? The objective of this research is to investigate if SNA metrics represent socio-technical relationships by examining if developer networks can be corroborated with developer perceptions. To measure developer perceptions, we developed an online survey\u00a0\u2026",
        "citations": 117
    },
    {
        "title": "Investigating the Relationship between Developer Collaboration and Software Security.",
        "id": "LFxEgGoAAAAJ:Se3iqnhoufwC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:Se3iqnhoufwC",
        "authors": [
            "Andrew Meneely"
        ],
        "pub_source": "",
        "pub_date": "2011/4/18",
        "description": "ABSTRACT MENEELY, ANDREW PHILIP. Investigating the Relationship between Developer Collaboration and Software Security.(Under th",
        "citations": 3
    },
    {
        "title": "Challenges for protecting the privacy of health information: required certification can leave common vulnerabilities undetected",
        "id": "LFxEgGoAAAAJ:UeHWp8X0CEIC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:UeHWp8X0CEIC",
        "authors": [
            "Ben Smith",
            "Andrew Austin",
            "Matt Brown",
            "Jason T King",
            "Jerrod Lankford",
            "Andrew Meneely",
            "Laurie Williams"
        ],
        "pub_source": "Proceedings of the second annual workshop on Security and privacy in medical and home-care systems",
        "pub_date": "2010/10/8",
        "description": "The use of electronic health record (EHR) systems by medical professionals enables the electronic exchange of patient data, yielding cost and quality of care benefits. The United States American Recovery and Reinvestment Act (ARRA) of 2009 provides up to $34 billion for meaningful use of certified EHR systems. But, will these certified EHR systems provide the infrastructure for secure patient data exchange? As a window into the ability of current and emerging certification criteria to expose security vulnerabilities, we performed exploratory security analysis on a proprietary and an open source EHR. We were able to exploit a range of common code-level and design-level vulnerabilities. These common vulnerabilities would have remained undetected by the 2011 security certification test scripts from the Certification Commission for Health Information Technology, the most widely used certification process for EHR\u00a0\u2026",
        "citations": 47
    },
    {
        "title": "Strengthening the empirical analysis of the relationship between Linus' Law and software security",
        "id": "LFxEgGoAAAAJ:Tyk-4Ss8FVUC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:Tyk-4Ss8FVUC",
        "authors": [
            "Andrew Meneely",
            "Laurie Williams"
        ],
        "pub_source": "Proceedings of the 2010 ACM-IEEE international symposium on empirical software engineering and measurement",
        "pub_date": "2010/9/16",
        "description": "Open source software is often considered to be secure because large developer communities can be leveraged to find and fix security vulnerabilities. Eric Raymond states Linus' Law as \"many eyes make all bugs shallow\", reasoning that a diverse set of perspectives improves the quality of a software product. However, at what point does the multitude of developers become \"too many cooks in the kitchen\", causing the system's security to suffer as a result? In a previous study, we quantified Linus' Law and \"too many cooks in the kitchen\" with developer activity metrics and found a statistical association between these metrics and security vulnerabilities in the Linux kernel. In the replication study reported in this paper, we performed our analysis on two additional projects: the PHP programming language and the Wireshark network protocol analyzer. We also updated our Linux kernel case study with 18 additional\u00a0\u2026",
        "citations": 70
    },
    {
        "title": "Evaluating complexity, code churn, and developer activity metrics as indicators of software vulnerabilities",
        "id": "LFxEgGoAAAAJ:9yKSN-GCB0IC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:9yKSN-GCB0IC",
        "authors": [
            "Yonghee Shin",
            "Andrew Meneely",
            "Laurie Williams",
            "Jason A Osborne"
        ],
        "pub_source": "IEEE transactions on software engineering",
        "pub_date": "2010/9/2",
        "description": "Security inspection and testing require experts in security who think like an attacker. Security experts need to know code locations on which to focus their testing and inspection efforts. Since vulnerabilities are rare occurrences, locating vulnerable code locations can be a challenging task. We investigated whether software metrics obtained from source code and development history are discriminative and predictive of vulnerable code locations. If so, security experts can use this prediction to prioritize security inspection and testing efforts. The metrics we investigated fall into three categories: complexity, code churn, and developer activity metrics. We performed two empirical case studies on large, widely used open-source projects: the Mozilla Firefox web browser and the Red Hat Enterprise Linux kernel. The results indicate that 24 of the 28 metrics collected are discriminative of vulnerabilities for both projects. The\u00a0\u2026",
        "citations": 718
    },
    {
        "title": "Improving developer activity metrics with issue tracking annotations",
        "id": "LFxEgGoAAAAJ:qjMakFHDy7sC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:qjMakFHDy7sC",
        "authors": [
            "Andrew Meneely",
            "Mackenzie Corcoran",
            "Laurie Williams"
        ],
        "pub_source": "Proceedings of the 2010 ICSE Workshop on Emerging Trends in Software Metrics",
        "pub_date": "2010/5/4",
        "description": "Understanding and measuring how groups of developers collaborate on software projects can provide valuable insight into software quality and the software development process. Current practices of measuring developer collaboration (e.g. with social network analysis) usually employ metrics based on version control change log data to determine who is working on which part of the system. Version control change logs, however, do not tell the whole story. Information about the collaborative problem-solving process is also documented in the issue tracking systems that record solutions to failures, feature requests, or other development tasks. To enrich the data gained from version control change logs, we propose two annotations to be used in issue tracking systems: solution originator and solution approver. We examined the online discussions of 602 issues from the OpenMRS healthcare web application\u00a0\u2026",
        "citations": 40
    },
    {
        "title": "Protection poker: The new software security\" game\"",
        "id": "LFxEgGoAAAAJ:_FxGoFyzp5QC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:_FxGoFyzp5QC",
        "authors": [
            "Laurie Williams",
            "Andrew Meneely",
            "Grant Shipley"
        ],
        "pub_source": "IEEE Security & Privacy",
        "pub_date": "2010/3/18",
        "description": "Without infinite resources, software development teams must prioritize security fortification efforts to prevent the most damaging attacks. The Protection Poker \"game\" is a collaborative means for guiding this prioritization and has the potential to improve software security practices and team software security knowledge.",
        "citations": 96
    },
    {
        "title": "On the use of issue tracking annotations for improving developer activity metrics",
        "id": "LFxEgGoAAAAJ:WF5omc3nYNoC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:WF5omc3nYNoC",
        "authors": [
            "Andrew Meneely",
            "Laurie Williams"
        ],
        "pub_source": "Advances in Software Engineering",
        "pub_date": "2010",
        "description": "Understanding and measuring how teams of developers collaborate on software projects can provide valuable insight into the software development process. Currently, researchers and practitioners measure developer collaboration with social networks constructed from version control logs. Version control change logs, however, do not tell the whole story. The collaborative problem\u2010solving process is also documented in the issue tracking systems that record solutions to failures, feature requests, or other development tasks. We propose two annotations to be used in issue tracking systems: solution originator and solution approver. We annotated which developers were originators or approvers of the solution to 602 issues from the OpenMRS healthcare system. We used these annotations to augment the version control logs and found 47 more contributors to the OpenMRS project than the original 40 found in the\u00a0\u2026",
        "citations": 6
    },
    {
        "title": "Software metrics validation criteria: A systematic literature review",
        "id": "LFxEgGoAAAAJ:W7OEmFMy1HYC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:W7OEmFMy1HYC",
        "authors": [
            "Andrew Meneely",
            "Ben Smith",
            "Laurie Ann Williams"
        ],
        "pub_source": "North Carolina State University. Dept. of Computer Science",
        "pub_date": "2010",
        "description": "Context: Researchers proposing a new metric have the burden of proof to demonstrate to the research community that the metric is acceptable in its intended use. This burden of proof is provided through the multi-faceted, scientific, and objective process of software metrics validation. Over the last 40 years, however, researchers have debated what constitutes a \u201cvalid\u201d metric.Aim: The debate over what constitutes a \u201cvalid\u201d metric centers on software metrics validation criteria. The objective of this paper is to guide researchers in making sound contributions to the field of software engineering metrics by providing a practical summary of the metrics validation criteria found in the academic literature.Method: We conducted a systematic literature review that began with 2,288 papers and ultimately focused on 20 papers. After extracting 47 unique validation criteria from these 20 papers, we performed a comparative analysis to explore the relationships amongst the criteria.Results: Our 47 validation criteria represent a diverse view of what constitutes a valid metric. We categorized each validation criterion into three categories and nine sub-categories. We present an analysis of the conflicts, common themes, and philosophical motivations behind the validation criteria.Conclusions: Although the 47 validation criteria are not conflict-free, the diversity of motivations and philosophies behind the validation criteria indicates that metrics validation is complex. We have determined in this paper that, rather than arbitrarily choosing validation criteria for each metric, researchers should choose criteria that confirm that the metric is appropriate for its intended use\u00a0\u2026",
        "citations": 16
    },
    {
        "title": "Secure open source collaboration: an empirical study of linus' law",
        "id": "LFxEgGoAAAAJ:u-x6o8ySG0sC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:u-x6o8ySG0sC",
        "authors": [
            "Andrew Meneely",
            "Laurie Williams"
        ],
        "pub_source": "Proceedings of the 16th ACM conference on Computer and communications security",
        "pub_date": "2009/11/9",
        "description": "Open source software is often considered to be secure. One factor in this confidence in the security of open source software lies in leveraging large developer communities to find vulnerabilities in the code. Eric Raymond declares Linus' Law \"Given enough eyeballs, all bugs are shallow.\" Does Linus' Law hold up ad infinitum? Or, can the multitude of developers become \"too many cooks in the kitchen\", causing the system's security to suffer as a result? In this study, we examine the security of an open source project in the context of developer collaboration. By analyzing version control logs, we quantified notions of Linus' Law as well as the \"too many cooks in the kitchen\" viewpoint into developer activity metrics. We performed an empirical case study by examining correlations between the known security vulnerabilities in the open source Red Hat Enterprise Linux 4 kernel and developer activity metrics. Files\u00a0\u2026",
        "citations": 173
    },
    {
        "title": "On preparing students for distributed software development with a synchronous, collaborative development platform",
        "id": "LFxEgGoAAAAJ:d1gkVwhDpl0C",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:d1gkVwhDpl0C",
        "authors": [
            "Andrew Meneely",
            "Laurie Williams"
        ],
        "pub_source": "Proceedings of the 40th ACM technical symposium on Computer science education",
        "pub_date": "2009/3/4",
        "description": "Working remotely is becoming the norm for both professionals and students alike. Software development has become a global industry due to outsourcing, teleworking, flex time, and companies' desire to use the best and/or most economical talent regardless of where that talent is located. Professionals are not alone because students usually work from home despite having sufficient resources on campus. In this paper we share our experiences from using Jazz, a synchronous, collaborative development platform, with our inevitably distributed software engineering students. Eleven students optionally used the tool while working on a five-week team project. Students primarily used the version control, chat, and work item features in Jazz. We collected their reactions in retrospective essays and found that all Jazz students supported using Jazz in future semesters of the course. We also examined grade differences and\u00a0\u2026",
        "citations": 40
    },
    {
        "title": "Protection poker: Structuring software security risk assessment and knowledge transfer",
        "id": "LFxEgGoAAAAJ:zYLM7Y9cAGgC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:zYLM7Y9cAGgC",
        "authors": [
            "Laurie Williams",
            "Michael Gegick",
            "Andrew Meneely"
        ],
        "pub_source": "Engineering Secure Software and Systems: First International Symposium ESSoS 2009, Leuven, Belgium, February 4-6, 2009. Proceedings 1",
        "pub_date": "2009",
        "description": "Discovery of security vulnerabilities is on the rise. As a result, software development teams must place a higher priority on preventing the injection of vulnerabilities in software as it is developed. Because the focus on software security has increased only recently, software development teams often do not have expertise in techniques for identifying security risk, understanding the impact of a vulnerability, or knowing the best mitigation strategy. We propose the Protection Poker activity as a collaborative and informal form of misuse case development and threat modeling that plays off the diversity of knowledge and perspective of the participants. An excellent outcome of Protection Poker is that security knowledge passed around the team. Students in an advanced undergraduate software engineering course at North Carolina State University participated in a Protection Poker session conducted as a laboratory\u00a0\u2026",
        "citations": 28
    },
    {
        "title": "Predicting failures with developer networks and social network analysis",
        "id": "LFxEgGoAAAAJ:u5HHmVD_uO8C",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:u5HHmVD_uO8C",
        "authors": [
            "Andrew Meneely",
            "Laurie Williams",
            "Will Snipes",
            "Jason Osborne"
        ],
        "pub_source": "Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of software engineering",
        "pub_date": "2008/11/9",
        "description": "Software fails and fixing it is expensive. Research in failure prediction has been highly successful at modeling software failures. Few models, however, consider the key cause of failures in software: people. Understanding the structure of developer collaboration could explain a lot about the reliability of the final product. We examine this collaboration structure with the developer network derived from code churn information that can predict failures at the file level. We conducted a case study involving a mature Nortel networking product of over three million lines of code. Failure prediction models were developed using test and post-release failure data from two releases, then validated against a subsequent release. One model's prioritization revealed 58% of the failures in 20% of the files compared with the optimal prioritization that would have found 61% in 20% of the files, indicating that a significant correlation exists\u00a0\u2026",
        "citations": 338
    },
    {
        "title": "ROSE: a repository of education-friendly open-source projects",
        "id": "LFxEgGoAAAAJ:2osOgNQ5qMEC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:2osOgNQ5qMEC",
        "authors": [
            "Andrew Meneely",
            "Laurie Williams",
            "Edward F Gehringer"
        ],
        "pub_source": "Proceedings of the 13th annual conference on Innovation and technology in computer science education",
        "pub_date": "2008/6/30",
        "description": "Open-source project artifacts can be used to inject realism into software engineering courses or lessons on open-source software development. However, the use of open-source projects presents challenges for both educators and for students. Educators must search for projects that meet the constraints of their classes, and often must negotiate the scope and terms of the project with project managers. For students, many available open-source projects have a steep learning curve that inhibits them from making significant contributions to the project and benefiting from a \"realistic\" experience. To alleviate these problems and to encourage cross-institution collaboration, we have created the Repository for Open Software Education (ROSE) and have contributed three open-source projects intended for an undergraduate computer science or software engineering course. The projects in ROSE are education-friendly in\u00a0\u2026",
        "citations": 35
    },
    {
        "title": "iTrust medical care requirements specification",
        "id": "LFxEgGoAAAAJ:5icHVeHT4IsC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:5icHVeHT4IsC",
        "authors": [
            "Laurie Williams",
            "Tao Xie",
            "Andy Meneely",
            "Lauren Hayward",
            "Jason King"
        ],
        "pub_source": "Versions of September 3rd 2010",
        "pub_date": "2008",
        "citations": 6
    },
    {
        "title": "Evaluating a suite of developer activity metrics as measures of security vulnerabilities",
        "id": "LFxEgGoAAAAJ:roLk4NBRz8UC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:roLk4NBRz8UC",
        "authors": [
            "Andrew Meneely",
            "Laurie Ann Williams"
        ],
        "pub_source": "North Carolina State University. Dept. of Computer Science",
        "pub_date": "2008",
        "description": "Deploying vulnerable software can be costly both in terms of patches and security breaches. Since software development primarily involves people, researchers are increasingly analyzing version control data to observe developer collaboration and contribution. We conducted a case study of the Linux kernel to evaluate a suite of developer activity metrics for the purpose of predicting security vulnerabilities. Our suite includes centrality and cluster metrics from network analysis of version control data. Our results support the hypothesis that source code files which have been developed by multiple clusters of developers are likely to be vulnerable. Furthermore, source code files are likely to be vulnerable when changed by many developers who themselves have made many changes to other files. Our results indicate that developer metrics predict vulnerabilities, but may be more likely to perform better in the presence of other code or process metrics.",
        "citations": 1
    },
    {
        "title": "Jazz Sangam: A real-time tool for distributed pair programming on a team development platform",
        "id": "LFxEgGoAAAAJ:Y0pCki6q_DkC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:Y0pCki6q_DkC",
        "authors": [
            "John Vijay Sena Devide",
            "Andrew Meneely",
            "Chih-Wei Ho",
            "Laurie Williams",
            "Michael Devetsikiotis"
        ],
        "pub_source": "Proceedings of Infrastructure for Research on Collaborative Software Engineering (IReCoSE)",
        "pub_date": "2008",
        "description": "Pair programming has proven to be a useful technique for developing high quality code while sharing knowledge throughout a team. Rapid global dispersion of software development teams, however, makes co-located pair programming a challenge, motivating the need for development tools tailored specifically for distributed pair programming. Previously, the Sangam Eclipse plug-in was developed to support distributed pair programming. More recently, the Jazz collaborative software development platform was built to support team communication and the sharing of life-cycle resources and to integrate a variety of disparate tools used by team members. We have ported Sangam to the Jazz platform to enable teams to pair program within their integrated team environment. In this paper, we describe Jazz Sangam, highlight the choices that lead to Sangam\u2019s current design, and discuss how Jazz Sangam can improve the distributed pair programming experience.",
        "citations": 17
    },
    {
        "title": "On Increasing System Test Effectiveness through a Test Case Prioritization Model Using Static Metrics and System Failure Data",
        "id": "LFxEgGoAAAAJ:eQOLeE2rZwMC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:eQOLeE2rZwMC",
        "authors": [
            "Laurie Williams",
            "Will Snipes",
            "Andrew Meneely"
        ],
        "pub_source": "Reliability Analysis of System Failure Data Workshop",
        "pub_date": "2007",
        "description": "System testing is the last phase before the product is delivered for customer use and thus represents the last opportunity for verifying that the system functions correctly and as desired by customers. System test is time consuming in that it involves configuring and testing multiple complete, integrated systems (including hardware, operating system, and cooperating and coexisting applications) that are representative of a subset of customer environments. As a result, prioritizing the execution order of system test cases to maximize system test effectiveness would be beneficial. We are developing a statistical test case prioritization model that uses static metrics and system failure data with the goal of improving system test effectiveness.",
        "citations": 3
    },
    {
        "title": "Fifteen compilers in fifteen days",
        "id": "LFxEgGoAAAAJ:YsMSGLbcyi4C",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:YsMSGLbcyi4C",
        "authors": [
            "Jeremy D Frens",
            "Andrew Meneely"
        ],
        "pub_source": "ACM SIGCSE Bulletin",
        "pub_date": "2006/3/3",
        "description": "Traditional approaches to semester-long projects in compiler courses force students to implement the early stages of a compiler in depth; since many students fall behind, they have little opportunity to implement the back end. Consequently, students have a deep knowledge of early material and no knowledge of latter material. We propose an approach based on incremental development and test-driven development; this approach solves the emphasis problem, provides experience with useful tools, and allows for such a course to be taught in a three or four weeks.",
        "citations": 5
    },
    {
        "title": "15 Compilers in 15 Days",
        "id": "LFxEgGoAAAAJ:fbc8zXXH2BUC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:fbc8zXXH2BUC",
        "authors": [
            "Jeremy D Frens",
            "Andrew Meneely"
        ],
        "pub_source": "",
        "pub_date": "",
        "description": "15 Compilers in 15 Days Page 1 Demonstration #1 What Did We Just See? Demonstration #2 \nWhat Did We Just See? Conclusions 15 Compilers in 15 Days Jeremy D. Frens & Andrew \nMeneely Calvin College SIGCSE 2006 Jeremy D. Frens & Andrew Meneely 15 Compilers in \n15 Days Page 2 Demonstration #1 What Did We Just See? Demonstration #2 What Did We \nJust See? Conclusions Compilers and a January Term January Term (aka \u201cInterim\u201d) Fifteen \nclass days; three weeks. Three hours per day in-class. Only course taken by students. Usually \npass/fail. Our independant study Fifteen weeks. Four hours per week in-class. Part of normal \nsemester. Taken for a grade. Jeremy D. Frens & Andrew Meneely 15 Compilers in 15 Days \nPage 3 Demonstration #1 What Did We Just See? Demonstration #2 What Did We Just See? \nConclusions Ready, set, go! 1 compiler in 10 minutes! Input: file with an integer. Ouput: file \u2026"
    },
    {
        "title": "Pragmatic Characteristics of Security Conversations",
        "id": "LFxEgGoAAAAJ:Aul-kAQHnToC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:Aul-kAQHnToC",
        "authors": [
            "Benjamin S Meyers",
            "Nuthan Munaiah",
            "Andrew Meneely",
            "Emily Prud\u2019hommeaux"
        ],
        "pub_source": "",
        "pub_date": "",
        "description": "Experts suggest that engineering secure software requires a defensive mindset to be ingrained in developer culture, which could be reflected in conversation. But what does a conversation about software security in a real project look like? Linguists analyze a wide array of characteristics: lexical, syntactic, semantic, and pragmatic. Pragmatics focus on identifying the style and tone of the author\u2019s language. If security requires a different mindset, then perhaps this would be reflected in the conversations\u2019 pragmatics. Our goal is to characterize the pragmatic features of conversations about security so that developers can be more informed about communication strategies regarding security concerns. We collected and annotated a corpus of conversations from 415,041 bug reports in the Chromium project. We examined five linguistic metrics related to pragmatics: formality, informativeness, implicature, politeness, and uncertainty. Our initial exploration into these data show that pragmatics plays a role, however small, in security conversations. These results indicate that the area of linguistic analysis shows promise in automatically identifying effective security communication strategies."
    },
    {
        "title": "SEAD 2019 Organization",
        "id": "LFxEgGoAAAAJ:_OXeSy2IsFwC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:_OXeSy2IsFwC",
        "authors": [
            "Matthias Galster",
            "Mehdi Mirakhorli",
            "Laurie Williams",
            "Daniela S Cruzes SINTEF",
            "Karim Ali",
            "Sebastien Bardin",
            "France Haipeng Cai",
            "Mohammad Ghafari",
            "Abbas Heydarnoori",
            "Eunsuk Kang",
            "Rick Kuhn",
            "Sam Malek",
            "Andrew Meneely",
            "Awais Rashid",
            "Joanna CS Santos",
            "Selma Suloglu",
            "Jinqiu Yang"
        ],
        "pub_source": "",
        "pub_date": "",
        "description": "SEAD 2019 Organization Toggle navigation IEEE Computer Society Digital Library Jobs Tech \nNews Resource Center Press Room Advertising About Us IEEE IEEE Computer Society IEEE \nComputer Society Digital Library My Subscriptions Magazines Journals Conference \nProceedings Institutional Subscriptions IEEE IEEE Computer Society More Jobs Tech News \nResource Center Press Room Advertising About Us Cart All Advanced Search Conference \nCover Image Download 1.Home 2.Proceedings 3.asew 2019 SEAD 2019 Organization 2019, \npp. 22-22, DOI Bookmark: 10.1109/ASEW.2019.00012 Keywords Authors Abstract Provides a \nlisting of current committee members and society officers. SEAD 2019 Organization Organizing \nCommittee Matthias Galster University of Canterbury, New Zealand Mehdi Mirakhorli Rochester \nInstitute of Technology, United States Laurie Williams North Carolina State University, \u2026"
    },
    {
        "title": "RCoSE-DDrEE 2019",
        "id": "LFxEgGoAAAAJ:nZcligLrVowC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:nZcligLrVowC",
        "authors": [
            "Nuthan Munaiah",
            "Andrew Meneely",
            "Robert Chatley",
            "Jan Ole Johanssen",
            "Lara Marie Reimer"
        ],
        "pub_source": "",
        "pub_date": "",
        "description": "Table of Contents Page 1 2019 IEEE/ACM Joint 4th International Workshop on Rapid Continuous \nSoftware Engineering and 1st International Workshop on Data-Driven Decisions, Experimentation \nand Evolution (RCoSE/DDrEE) RCoSE-DDrEE 2019 Table of Contents Message from the \nRCoSE-DDrEE 2019 Workshop Organizers vii RCoSE-DDrEE 2019 Organizing Committee \nviii RCoSE-DDrEE 2019 Committees ix Technical Papers Data-Driven Insights from \nVulnerability Discovery Metrics 1 Nuthan Munaiah (Rochester Institute of Technology) and \nAndrew Meneely (Rochester Institute of Technology) Supporting the Developer Experience \nwith Production Metrics 8 Robert Chatley (Imperial College London) Continuous Thinking \nAloud 12 Jan Ole Johanssen (Technical University of Munich), Lara Marie Reimer (Technical \nUniversity of Munich), and Bernd Bruegge (Technical University of Munich) Hypotheses \u2026"
    },
    {
        "title": "Program Committee for SEAD 2018",
        "id": "LFxEgGoAAAAJ:a3BOlSfXSfwC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:a3BOlSfXSfwC",
        "authors": [
            "Daniela Cruzes",
            "Jens Dietrich",
            "Rick Kuhn",
            "Heather Lipford",
            "Sam Malek",
            "Andrew Meneely",
            "Patrick M\u00e4der",
            "Matthias Naab",
            "Riccardo Scandariato",
            "Katy Tarrit",
            "Laurie Williams"
        ],
        "pub_source": "",
        "pub_date": "",
        "description": "Provides a listing of current committee members and society officers."
    },
    {
        "title": "Beyond the Attack Surface",
        "id": "LFxEgGoAAAAJ:yMeIxYmEMEAC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:yMeIxYmEMEAC",
        "authors": [
            "Nuthan Munaiah",
            "Andrew Meneely"
        ],
        "pub_source": "Year unknown",
        "pub_date": "",
        "description": "When reasoning about software security, researchers and practitioners use the phrase \u201cattack surface\u201d as a metaphor for risk. Enumerate and minimize the ways attackers can break in then risk is reduced and the system is better protected, the metaphor says. But software systems are much more complicated than their surfaces. We propose functionand file-level attack surface metrics\u2014proximity and risky walk\u2014that enable fine-grained risk assessment. Our risky walk metric is highly configurable: we use PageRank on a probability-weighted call graph to simulate attacker behavior of finding or exploiting a vulnerability. We provide evidence-based guidance for deploying these metrics, including an extensive parameter tuning study. We conducted an empirical study on two large open source projects, FFmpeg and Wireshark, to investigate the potential correlation between our metrics and historical post-release\u00a0\u2026",
        "citations": 2
    },
    {
        "title": "Academic Employment",
        "id": "LFxEgGoAAAAJ:OR75R8vi5nAC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:OR75R8vi5nAC",
        "authors": [
            "ANDREW MENEELY"
        ],
        "pub_source": "",
        "pub_date": "",
        "description": "a. Summer 2010. Software Metrics Intern. Cisco, Inc. Developed social network analysis metrics for assessing software development teams. Empirically analyzed the development history of several large networking projects currently in production. b. Summer 2009. Cyber Security Intern. Applied Research Associates. Conceived, designed, and prototyped a counterintelligence risk management system. Lead the development team as manager and lead developer. Presented work to potential customers. c. Summers 2005 and 2006. Software Development for Informatics Intern. Pfizer Pharmaceuticals Global R&D. Developed web application software for pharmaceutical research, including robotic laboratory control systems and biological sample inventory systems. Worked with customers, managers, and colleagues within a software development process."
    },
    {
        "title": "MSR 2010 challenge Committee",
        "id": "LFxEgGoAAAAJ:_kc_bZDykSQC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:_kc_bZDykSQC",
        "authors": [
            "Israel Herraiz",
            "Emily Hill",
            "Annie Ying",
            "Emad Shihab",
            "Zhen Ming Jiang",
            "Rahul Premraj",
            "Irwin Kwan",
            "Lile Hattori",
            "Adrian Schr\u00f6ter",
            "Raza Abbas",
            "Rui Abreu",
            "Ashwini Auradkar",
            "Andrew Austin",
            "Nicolas Bettenburg",
            "Christian Bird",
            "Amancio Bouza",
            "Martin Burger",
            "Raymond Buse",
            "Barth\u00e9l\u00e9my Dagenais",
            "Valentin Dallmeier",
            "Beat Fluri",
            "Patrick Francis",
            "Zachary Fry",
            "Giacomo Ghezzi",
            "Anja Guzzi",
            "Sonia Haiduc",
            "Christina Hamada",
            "Maen Hammad",
            "Kim Herzig",
            "Abram Hindle",
            "Pieter Hooimeijer",
            "Yasutaka Kamei",
            "Madhuri Marri",
            "Shinsuke Matsumoto",
            "Tomko Matsumura",
            "Andrew Meneely",
            "Yana Mileva",
            "Masao Ohira",
            "Rahul Pandita",
            "Boccuzzo Sandro",
            "David Schuler",
            "Yonhee Shin",
            "Ben Smith",
            "Yoonki Song",
            "Kunal Taneja",
            "Suresh Thummalapenta",
            "Wei Wang",
            "Xusheng Xiao"
        ],
        "pub_source": "",
        "pub_date": "",
        "description": "Provides a listing of current committee members."
    },
    {
        "title": "IWESEP 2013 Workshop",
        "id": "LFxEgGoAAAAJ:ULOm3_A8WrAC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:ULOm3_A8WrAC",
        "authors": [
            "MCIS Bram Adams",
            "Sosuke Amasaki",
            "Christian Bird",
            "Kyohei Fushida",
            "Sonia Haiduc",
            "Yasuhiro Hayase",
            "Shinpei Hayashi",
            "Israel Herraiz",
            "Takashi Ishio",
            "Yasutaka Kamei",
            "Yuichiro Kanzaki",
            "Chris Lokan",
            "Yuki Manabe",
            "Andrew Meneely",
            "Ayse Tosun Misirli",
            "Osamu Mizuno",
            "Meiyappan Nagappan",
            "Xiao Qu",
            "Peter Rigby",
            "Koji Toda",
            "Hidetake Uwano",
            "Thomas Zimmermann",
            "Weiyi Shang",
            "Yujuan Jiang"
        ],
        "pub_source": "",
        "pub_date": "",
        "description": "Provides a listing of current committee members and society officers."
    },
    {
        "title": "IWESEP 2012",
        "id": "LFxEgGoAAAAJ:5nxA0vEk-isC",
        "url": "https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=LFxEgGoAAAAJ:5nxA0vEk-isC",
        "authors": [
            "MCIS Bram Adams",
            "Sousuke Amasaki",
            "Christian Bird",
            "Ahmed E Hassan",
            "Hideaki Hata",
            "Yasuhiro Hayase",
            "Shinpei Hayashi",
            "Abram Hindle",
            "Takashi Ishio",
            "Yasutaka Kamei",
            "Yuichiro Kanzaki",
            "Lucas Layman",
            "David Lo",
            "Yuki Manabe",
            "Andrew Meneely",
            "Osamu Mizuno",
            "Meiyappan Nagappan",
            "Xiao Qu",
            "Romain Robbes",
            "Emad Shihab",
            "Suresh Thummalapenta",
            "Koji Toda",
            "Hidetake Uwano",
            "Thomas Zimmermann"
        ],
        "pub_source": "",
        "pub_date": "",
        "description": "Provides a listing of current committee members."
    }
]